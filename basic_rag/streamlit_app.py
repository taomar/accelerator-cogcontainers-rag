import streamlit as st
from query_rag import generate_response, search_documents, detect_language
import ollama

# ğŸ¨ Streamlit UI Setup
st.set_page_config(page_title="RAG System", layout="wide")

st.title("ğŸ” ğŸ§  AI-Powered RAG System: Deployable Offline, On-Premise, or Any Cloud ğŸš€")
st.markdown("This system retrieves relevant documents using **vector search** (Qdrant) and **BM25 keyword matching**, then generates a response using **large language models (LLMs)**.")

# â„¹ï¸ Sidebar with Model & Retrieval Info
with st.sidebar:
    st.header("â„¹ï¸ System Information")
    st.markdown("""
    - **Retrieval Method**: Hybrid (Vector Search + BM25)
    - **Vector Database**: [Qdrant](https://qdrant.tech/)
    - **Embedding Models**:
      - `qwen2.5:0.5b` for English (Ollama)
      - `gemma2:2b` for Arabic (Ollama)
    - **Reranking**: [BGE-M3](https://huggingface.co/BAAI/bge-m3) for relevance tuning
    """)

# ğŸŒ Dropdown for language selection
language_option = st.selectbox("ğŸŒ Choose Language:", ["Auto-Detect", "Arabic", "English"])

# ğŸ’¬ Query input
query_text = st.text_input("ğŸ’¬ Enter your query:")

# ğŸ” Search button
if st.button("ğŸ” Search"):
    if query_text:
        # ğŸ” Detect or use user-selected language
        if language_option == "Auto-Detect":
            language = detect_language(query_text)
            st.info(f"ğŸŒ **Detected Language:** `{language.capitalize()}`")
        else:
            language = "arabic" if language_option.lower() == "arabic" else "english"

        # ğŸ” Perform retrieval
        retrieved_docs = search_documents(query_text, language)
        
        # Display retrieved documents
        st.subheader("ğŸ“„ Retrieved Documents")
        if retrieved_docs:
            for idx, doc in enumerate(retrieved_docs):
                st.markdown(f"**{idx + 1}.** {doc['text']} _(Score: {doc['score']:.2f})_")
        else:
            st.warning("âš ï¸ No relevant documents found!")

        # ğŸ¤– Generate AI response
        st.subheader("ğŸ¤– AI Response")
        ai_response = generate_response(query_text)
        st.write(ai_response)

        # ğŸ“Œ Additional Debug Info (Expandable)
        with st.expander("ğŸ“Š Debug Info: Retrieval & Model Selection"):
            st.markdown(f"""
            - **Selected Model**: `{ "gemma2:2b" if language == "arabic" else "qwen2.5:0.5b" }`
            - **Documents Retrieved**: `{len(retrieved_docs)}` 
            - **BM25 Ranking Applied**: `âœ… Yes`
            - **Vector Search**: `âœ… Yes`
            """)
    else:
        st.warning("âš ï¸ Please enter a query!")